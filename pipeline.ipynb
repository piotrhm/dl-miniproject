{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biw2wtIGuxoO"
   },
   "source": [
    "# ViT vs CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "BXqPNcOYA0-h"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from typing import List\n",
    "\n",
    "import BiT\n",
    "from ViT import modeling as ViT\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.Image import Image as Img\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xTIMaZBOuxoU"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T0ohcnnTgsdU"
   },
   "outputs": [],
   "source": [
    "def get_weights(path):\n",
    "  return np.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m61ZHyIUuxoV"
   },
   "source": [
    "## Prepare dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (384, 384)\n",
    "NORMALIZE_MEAN = (0.5, 0.5, 0.5)\n",
    "NORMALIZE_STD = (0.5, 0.5, 0.5)\n",
    "transforms = [\n",
    "              T.Resize(IMG_SIZE),\n",
    "              T.ToTensor(),\n",
    "              T.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n",
    "              ]\n",
    "\n",
    "transforms = T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ylaiH85zuxoW"
   },
   "outputs": [],
   "source": [
    "def load_images(dataset_path: str) -> List[Img]:\n",
    "    images = []\n",
    "    for filename in glob.glob(dataset_path + '/*.jpg'):\n",
    "        im=Image.open(filename).convert('RGB')\n",
    "        images.append(im)\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_vit(model_name='ViT-B_16', path='ViT-B_16.npz') -> nn.Module:\n",
    "    config = ViT.CONFIGS[model_name]\n",
    "    model = ViT.VisionTransformer(config, num_classes=1000, img_size=384)\n",
    "    model.load_from(get_weights(path))\n",
    "    return model.eval()\n",
    "\n",
    "def load_bit(model_name='BiT-M-R50x3', path='BiT-M-R50x3-ILSVRC2012.npz') -> nn.Module:\n",
    "    model = BiT.KNOWN_MODELS[model_name](head_size=1000)\n",
    "    model.load_from(get_weights(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9oHO_8vSuxoW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = load_images(dataset_path)\n",
    "vit = load_vit()\n",
    "bit = load_bit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Wxf-JQ6uxoW"
   },
   "source": [
    "## Compare predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensors = [transforms(img).unsqueeze(0) for img in images]\n",
    "\n",
    "def batches(img_tensors: List[torch.Tensor], batch_size=32):\n",
    "    rest = min(1, len(img_tensors) % batch_size)\n",
    "    for i in range(len(img_tensors) // batch_size + rest):\n",
    "        yield img_tensors[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "def predict_and_save(model, img_tensors: List[torch.Tensor], target_path: str, batch_size: int = 32):\n",
    "    outputs = []\n",
    "    for batch in batches(img_tensors, batch_size):\n",
    "        img_batch = torch.cat(batch)\n",
    "        with torch.no_grad():\n",
    "            output = model(img_batch)\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        outputs.append(output)\n",
    "    outputs = torch.cat(outputs)\n",
    "    pickle.dump(outputs.cpu(), open(target_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main computational costs\n",
    "predict_and_save(vit, img_tensors, target_path='vit_output.pickle', batch_size=5)\n",
    "predict_and_save(bit, img_tensors, target_path='bit_output.pickle', batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_labels_list(outputs: torch.Tensor) -> List[List[str]]:\n",
    "    # outputs.shape == (images_no, class_no)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_uninteresting(vit_top_labels: List[List[str]], bit_top_labels: List[List[str]]) \\\n",
    "    -> Tuple[List[PIL.Image.Image], List[List[str]], List[List[str]]]:\n",
    "    # return interesting_images, corresponding_vit_labels, corresponding_bit_labels\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_predictions(interesting_images, corresponding_vit_labels, corresponding_bit_labels):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_outputs = pickle.load(open('vit_output.pickle', 'rb'))\n",
    "bit_outputs = pickle.load(open('bit_output.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_predictions = extract_top_labels_list(vit_outputs)\n",
    "bit_predictions = extract_top_labels_list(bit_outputs)\n",
    "images_interesing, vit_interesting, bit_interesting = filter_uninteresting(vit_predictions, bit_predictions)\n",
    "show_all_predictions(images_interesing, vit_interesting, bit_interesting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaoxjYYQuxoW"
   },
   "source": [
    "## Compare activation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWcdGXoCuxoX",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvYo8vuiuxoX"
   },
   "source": [
    "## Compare embeddings clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdcOhBGxuxoX",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def draw_clustering(embeddings, n_clusters, r=1.5):\n",
    "    clustering = AgglomerativeClustering(n_clusters).fit(np.array(embeddings))\n",
    "    colors = [np.random.rand(3,) for _ in range(n_clusters)]\n",
    "    labels = clustering.labels_\n",
    "    embeddings_2d = MDS().fit_transform(embeddings)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(17, 17))\n",
    "    \n",
    "    for i in range(embeddings_2d.shape[0]):\n",
    "        im = OffsetImage(images[i], zoom=0.1)\n",
    "        ab = AnnotationBbox(im, embeddings_2d[i], xycoords='data', frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        index = labels == i\n",
    "        points = []\n",
    "        for pt in embeddings_2d[index]:\n",
    "            points.append(pt)\n",
    "            points.append(pt + np.array((1, 1)) * r)\n",
    "            points.append(pt + np.array((1, -1)) * r)\n",
    "            points.append(pt + np.array((-1, 1)) * r)\n",
    "            points.append(pt + np.array((-1, -1)) * r)\n",
    "        points = np.array(points)\n",
    "        hull = ConvexHull(points)\n",
    "        hull_points = points[hull.vertices]\n",
    "        x_hull = np.append(hull_points[:, 0], hull_points[:, 0][0])\n",
    "        y_hull = np.append(hull_points[:, 1], hull_points[:, 1][0])\n",
    "        plt.fill(x_hull, y_hull, alpha=0.3, c=colors[i])\n",
    "        \n",
    "    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "images = images_interesting\n",
    "img_tensors = [transforms(img).unsqueeze(0) for img in images]\n",
    "img_batch = torch.cat(img_tensors)\n",
    "assert img_batch.shape[0] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vit_embeddings = vit.transformer(img_batch)[0][:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "base = list(bit.children())[:-1]\n",
    "pooling = list(bit.head.children())[:-1]\n",
    "bit_pruned = nn.Sequential(*base, *pooling)\n",
    "with torch.no_grad():\n",
    "    bit_embeddings = bit_pruned(img_batch).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "draw_clustering(vit_embeddings, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "draw_clustering(bit_embeddings, 8, r=8)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
