{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biw2wtIGuxoO"
   },
   "source": [
    "# ViT vs CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXqPNcOYA0-h"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from typing import List\n",
    "\n",
    "import BiT\n",
    "from ViT import modeling as ViT\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.Image import Image as Img\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTIMaZBOuxoU"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'data/cats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0ohcnnTgsdU"
   },
   "outputs": [],
   "source": [
    "def get_weights(path):\n",
    "  return np.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m61ZHyIUuxoV"
   },
   "source": [
    "## Prepare dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (384, 384)\n",
    "NORMALIZE_MEAN = (0.5, 0.5, 0.5)\n",
    "NORMALIZE_STD = (0.5, 0.5, 0.5)\n",
    "transforms = [\n",
    "              T.Resize(IMG_SIZE),\n",
    "              T.ToTensor(),\n",
    "              T.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n",
    "              ]\n",
    "\n",
    "transforms = T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylaiH85zuxoW"
   },
   "outputs": [],
   "source": [
    "def load_images(dataset_path: str) -> List[Img]:\n",
    "    images = []\n",
    "    for filename in glob.glob(dataset_path + '/*.jpg'):\n",
    "        im=Image.open(filename).convert('RGB')\n",
    "        images.append(im)\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_vit(model_name='ViT-B_16', path='ViT-B_16.npz') -> nn.Module:\n",
    "    config = ViT.CONFIGS[model_name]\n",
    "    model = ViT.VisionTransformer(config, num_classes=1000, img_size=384)\n",
    "    model.load_from(get_weights(path))\n",
    "    return model.eval()\n",
    "\n",
    "def load_bit(model_name='BiT-M-R50x3', path='BiT-M-R50x3-ILSVRC2012.npz') -> nn.Module:\n",
    "    model = BiT.KNOWN_MODELS[model_name](head_size=1000)\n",
    "    model.load_from(get_weights(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oHO_8vSuxoW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = load_images(dataset_path)\n",
    "vit = load_vit()\n",
    "bit = load_bit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Wxf-JQ6uxoW"
   },
   "source": [
    "## Compare predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixCuHDCSuxoW"
   },
   "outputs": [],
   "source": [
    "imagenet_labels = dict(enumerate(open('ilsvrc2012_wordnet_lemmas.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(vit_predictions, bit_predictions):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs = GridSpec(nrows=3, ncols=2)\n",
    "\n",
    "    ax0 = fig.add_subplot(gs[1, 0])\n",
    "    ax0.axis(\"off\")\n",
    "    ax0.invert_yaxis()\n",
    "    ax0.text(0.1, 0.25, bit_predictions, verticalalignment=\"center\")\n",
    "    ax0.set_title(\"BiT prediction\")\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[2, 0])\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.text(0.1, 0.25, vit_predictions, verticalalignment=\"center\")\n",
    "    ax1.set_title(\"ViT prediction\")\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[:, 1])\n",
    "    ax2.imshow(img)\n",
    "    ax2.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions_comp(\n",
    "        vit_predictions_default, vit_predictions_new, \n",
    "        bit_predictions_default, bit_predictions_new):\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs = GridSpec(nrows=3, ncols=3)\n",
    "\n",
    "    ax0 = fig.add_subplot(gs[1, 0])\n",
    "    ax0.axis(\"off\")\n",
    "    ax0.invert_yaxis()\n",
    "    ax0.text(0.1, 0.25, bit_predictions_default, verticalalignment=\"center\")\n",
    "    ax0.set_title(\"BiT default prediction\")\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[2, 0])\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.text(0.1, 0.25, vit_predictions_default, verticalalignment=\"center\")\n",
    "    ax1.set_title(\"ViT default prediction\")\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[1, 1])\n",
    "    ax2.axis(\"off\")\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.text(0.1, 0.25, bit_predictions_new, verticalalignment=\"center\")\n",
    "    ax2.set_title(\"BiT new prediction\")\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[2, 1])\n",
    "    ax3.axis(\"off\")\n",
    "    ax3.invert_yaxis()\n",
    "    ax3.text(0.1, 0.25, vit_predictions_new, verticalalignment=\"center\")\n",
    "    ax3.set_title(\"ViT new prediction\")\n",
    "\n",
    "    ax4 = fig.add_subplot(gs[:, 2])\n",
    "    ax4.imshow(img)\n",
    "    ax4.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_pred = 3\n",
    "\n",
    "for img in images:\n",
    "    img_tensor = transforms(img).unsqueeze(0)\n",
    "\n",
    "    out_vit = vit(img_tensor)\n",
    "    out_bit = bit(img_tensor)\n",
    "\n",
    "    vit_predictions = [imagenet_labels[int(i)] for i in np.argsort(out_vit[0].detach().numpy())[0]]\n",
    "    bit_predictions = [imagenet_labels[int(i)] for i in np.argsort(out_bit.detach().numpy())[0]]\n",
    "\n",
    "    vit_predictions = ''.join(str(e) for e in vit_predictions[-no_pred:])\n",
    "    bit_predictions = ''.join(str(e) for e in bit_predictions[-no_pred:])\n",
    "    \n",
    "    show_predictions(vit_predictions, bit_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaoxjYYQuxoW"
   },
   "source": [
    "## Compare activation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWcdGXoCuxoX"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvYo8vuiuxoX"
   },
   "source": [
    "## Compare embeddings clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdcOhBGxuxoX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_clustering(embeddings, n_clusters, r=1.5):\n",
    "    clustering = AgglomerativeClustering(n_clusters).fit(np.array(embeddings))\n",
    "    colors = [np.random.rand(3,) for _ in range(n_clusters)]\n",
    "    labels = clustering.labels_\n",
    "    embeddings_2d = MDS().fit_transform(embeddings)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(17, 17))\n",
    "    \n",
    "    for i in range(embeddings_2d.shape[0]):\n",
    "        im = OffsetImage(images[i], zoom=0.1)\n",
    "        ab = AnnotationBbox(im, embeddings_2d[i], xycoords='data', frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        index = labels == i\n",
    "        points = []\n",
    "        for pt in embeddings_2d[index]:\n",
    "            points.append(pt)\n",
    "            points.append(pt + np.array((1, 1)) * r)\n",
    "            points.append(pt + np.array((1, -1)) * r)\n",
    "            points.append(pt + np.array((-1, 1)) * r)\n",
    "            points.append(pt + np.array((-1, -1)) * r)\n",
    "        points = np.array(points)\n",
    "        hull = ConvexHull(points)\n",
    "        hull_points = points[hull.vertices]\n",
    "        x_hull = np.append(hull_points[:, 0], hull_points[:, 0][0])\n",
    "        y_hull = np.append(hull_points[:, 1], hull_points[:, 1][0])\n",
    "        plt.fill(x_hull, y_hull, alpha=0.3, c=colors[i])\n",
    "        \n",
    "    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images\n",
    "img_tensors = [transforms(img).unsqueeze(0) for img in images]\n",
    "img_batch = torch.cat(img_tensors)\n",
    "assert img_batch.shape[0] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vit_embeddings = vit.transformer(img_batch)[0][:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = list(bit.children())[:-1]\n",
    "pooling = list(bit.head.children())[:-1]\n",
    "bit_pruned = nn.Sequential(*base, *pooling)\n",
    "with torch.no_grad():\n",
    "    bit_embeddings = bit_pruned(img_batch).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_clustering(vit_embeddings, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_clustering(bit_embeddings, 8, r=8)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
