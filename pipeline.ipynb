{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biw2wtIGuxoO"
   },
   "source": [
    "# ViT vs CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXqPNcOYA0-h"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from typing import List\n",
    "\n",
    "import BiT\n",
    "from ViT import modeling as ViT\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.Image import Image as Img\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTIMaZBOuxoU"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'data/cats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0ohcnnTgsdU"
   },
   "outputs": [],
   "source": [
    "def get_weights(path):\n",
    "  return np.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m61ZHyIUuxoV"
   },
   "source": [
    "## Prepare dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (384, 384)\n",
    "NORMALIZE_MEAN = (0.5, 0.5, 0.5)\n",
    "NORMALIZE_STD = (0.5, 0.5, 0.5)\n",
    "transforms = [\n",
    "              T.Resize(IMG_SIZE),\n",
    "              T.ToTensor(),\n",
    "              T.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n",
    "              ]\n",
    "\n",
    "transforms = T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylaiH85zuxoW"
   },
   "outputs": [],
   "source": [
    "def load_images(dataset_path: str) -> List[Img]:\n",
    "    images = []\n",
    "    for filename in glob.glob(dataset_path + '/*.jpg'):\n",
    "        im=Image.open(filename).convert('RGB')\n",
    "        images.append(im)\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_vit(model_name='ViT-B_16', path='ViT-B_16.npz') -> nn.Module:\n",
    "    config = ViT.CONFIGS[model_name]\n",
    "    model = ViT.VisionTransformer(config, num_classes=1000, img_size=384)\n",
    "    model.load_from(get_weights(path))\n",
    "    return model.eval()\n",
    "\n",
    "def load_bit(model_name='BiT-M-R50x3', path='BiT-M-R50x3-ILSVRC2012.npz') -> nn.Module:\n",
    "    model = BiT.KNOWN_MODELS[model_name](head_size=1000)\n",
    "    model.load_from(get_weights(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oHO_8vSuxoW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = load_images(dataset_path)\n",
    "vit = load_vit()\n",
    "bit = load_bit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Wxf-JQ6uxoW"
   },
   "source": [
    "## Compare predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.perturb_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = {\n",
    "    'BlackBoxMasking_1': BlackBoxMasking(layout=FixedShapeLayout(count=3, shape=(128, 128))),\n",
    "    'BlackBoxMasking_2': BlackBoxMasking(layout=FixedLayout(masks=[(0.4, 0.4, 0.6, 0.6)])),\n",
    "    'BlackBoxMasking_3': BlackBoxMasking(layout=FixedRatioLayout(count=20, ratio=1/30)),\n",
    "    'BlackBoxMasking_4': BlackBoxMasking(layout=GridLayout(perc=0.8, grid_size=8)),\n",
    "    'MedianMasking' : MedianMasking(layout=FixedShapeLayout(count=30, shape=(32,32))),\n",
    "    'BlurMasking_1': BlurMasking(layout=FixedShapeLayout(count=5, shape=(100, 100)), blur=16),\n",
    "    'BlurMasking_2': BlurMasking(layout=GridLayout(perc=0.7, grid_size=8), blur=16),\n",
    "}\n",
    "\n",
    "MASK_NO = len(MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(images: List[Img]) -> Tuple[List[Img], List[torch.Tensor]]:\n",
    "    out_images, out_raw = [], []\n",
    "    for img in images:\n",
    "        out_images.append(transforms(img).unsqueeze(0))\n",
    "        out_raw.append(img)\n",
    "        for _, func in MASK.items():\n",
    "            out_raw.append(func(img))\n",
    "            out_images.append(transforms(func(img)).unsqueeze(0))\n",
    "    return out_raw, out_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_raw, img_tensors = generate_dataset(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(img_raw, open(\"img_raw.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(img_tensors, open(\"img_tensor.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(img_tensors: List[torch.Tensor], batch_size=32):\n",
    "    rest = min(1, len(img_tensors) % batch_size)\n",
    "    for i in range(len(img_tensors) // batch_size + rest):\n",
    "        yield img_tensors[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "def predict_and_save(model, img_tensors: List[torch.Tensor], target_path: str, batch_size: int = 32):\n",
    "    outputs = []\n",
    "    for batch in batches(img_tensors, batch_size):\n",
    "        img_batch = torch.cat(batch)\n",
    "        with torch.no_grad():\n",
    "            output = model(img_batch)\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        outputs.append(output)\n",
    "    outputs = torch.cat(outputs)\n",
    "    pickle.dump(outputs.cpu(), open(target_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main computational costs\n",
    "predict_and_save(vit, img_tensors, target_path='vit_output.pickle', batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main computational costs 2\n",
    "predict_and_save(bit, img_tensors, target_path='bit_output.pickle', batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_labels = dict(enumerate(open('ilsvrc2012_wordnet_lemmas.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_labels_list(outputs: torch.Tensor, top_n) -> List[List[str]]:\n",
    "    out = []\n",
    "    for model_out in outputs:\n",
    "        out.append([imagenet_labels[int(i)] for i in np.argsort(model_out.detach().numpy())][-top_n:])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_labels = [\n",
    "    'tabby, tabby_cat\\n',\n",
    "    'tiger_cat\\n',\n",
    "    'Persian_cat\\n',\n",
    "    'Siamese_cat, Siamese\\n',\n",
    "    'Egyptian_cat\\n',\n",
    "    'cougar, puma, catamount, mountain_lion, painter, panther, Felis_concolor\\n',\n",
    "    'lynx, catamount\\n',\n",
    "    'leopard, Panthera_pardus\\n',\n",
    "    'snow_leopard, ounce, Panthera_uncia\\n',\n",
    "    'jaguar, panther, Panthera_onca, Felis_onca\\n',\n",
    "    'lion, king_of_beasts, Panthera_leo\\n',\n",
    "    'tiger, Panthera_tigris\\n',\n",
    "    'cheetah, chetah, Acinonyx_jubatus\\n',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_uninteresting(vit_top_labels: List[List[str]], bit_top_labels: List[List[str]], stats_on=True) \\\n",
    "    -> Tuple[List[PIL.Image.Image], List[List[str]], List[List[str]], List[int], List[int]]:\n",
    "    interesting_images, corresponding_vit_labels, corresponding_bit_label = [], [], []\n",
    "    stats_vit = [0] * 7\n",
    "    stats_bit = [0] * 7\n",
    "    for i in range(0, len(img_raw), MASK_NO+1):\n",
    "        for j in range(i+1, i+MASK_NO+1, 1):\n",
    "            vit_default_pred = set(vit_top_labels[i]) - set(forbidden_labels)\n",
    "            bit_default_pred = set(bit_top_labels[i]) - set(forbidden_labels)\n",
    "            vit_new_pred = set(vit_top_labels[j]) - set(forbidden_labels)\n",
    "            bit_new_pred = set(bit_top_labels[j]) - set(forbidden_labels)\n",
    "\n",
    "            if (vit_default_pred != vit_new_pred) or (bit_default_pred != bit_new_pred):    \n",
    "                interesting_images.append(img_raw[j])\n",
    "                corresponding_vit_labels.append((vit_top_labels[i], vit_top_labels[j]))\n",
    "                corresponding_bit_label.append((bit_top_labels[i], bit_top_labels[j]))\n",
    "                \n",
    "            if stats_on:\n",
    "                if (vit_default_pred != vit_new_pred) : stats_vit[(j-1)%MASK_NO] += 1\n",
    "                if (bit_default_pred != bit_new_pred) : stats_bit[(j-1)%MASK_NO] += 1\n",
    "\n",
    "    return interesting_images, corresponding_vit_labels, corresponding_bit_label, stats_bit, stats_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predict_compare(img, vit_labels, bit_labels):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs = GridSpec(nrows=3, ncols=3)\n",
    "    \n",
    "    labels_0 = ''.join(str(e) for e in bit_labels[0])\n",
    "    labels_1 = ''.join(str(e) for e in vit_labels[0])\n",
    "    labels_2 = ''.join(str(e) for e in bit_labels[1])\n",
    "    labels_3 = ''.join(str(e) for e in vit_labels[1])\n",
    "\n",
    "    ax0 = fig.add_subplot(gs[1, 0])\n",
    "    ax0.axis(\"off\")\n",
    "    ax0.invert_yaxis()\n",
    "    ax0.text(0.1, 0.25, labels_0, verticalalignment=\"center\")\n",
    "    ax0.set_title(\"BiT default prediction\")\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[2, 0])\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.text(0.1, 0.25, labels_1, verticalalignment=\"center\")\n",
    "    ax1.set_title(\"ViT default prediction\")\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[1, 1])\n",
    "    ax2.axis(\"off\")\n",
    "    ax2.invert_yaxis()\n",
    "    ax2.text(0.1, 0.25, labels_2, verticalalignment=\"center\")\n",
    "    ax2.set_title(\"BiT new prediction\")\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[2, 1])\n",
    "    ax3.axis(\"off\")\n",
    "    ax3.invert_yaxis()\n",
    "    ax3.text(0.1, 0.25, labels_3, verticalalignment=\"center\")\n",
    "    ax3.set_title(\"ViT new prediction\")\n",
    "\n",
    "    ax4 = fig.add_subplot(gs[:, 2])\n",
    "    ax4.imshow(img)\n",
    "    ax4.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_predictions(interesting_images, corresponding_vit_labels, corresponding_bit_labels):\n",
    "    for idx, img in enumerate(interesting_images):\n",
    "        print_predict_compare(img, corresponding_vit_labels[idx], corresponding_bit_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_outputs = pickle.load(open('vit_output.pickle', 'rb'))\n",
    "bit_outputs = pickle.load(open('bit_output.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vit_predictions = extract_top_labels_list(vit_outputs, 3)\n",
    "bit_predictions = extract_top_labels_list(bit_outputs, 3)\n",
    "images_fail, vit_fail, bit_fail, stats_bit, stats_vit = filter_uninteresting(vit_predictions, bit_predictions)\n",
    "show_all_predictions(images_fail, vit_fail, bit_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, mask in enumerate([*MASK.keys()]):\n",
    "    print(\"{:s} BIT failure rate: {:.2f} VIT failure rate: {:.2f}\".format(\n",
    "        mask, float(stats_bit[idx]/img_size), float(stats_vit[idx]/img_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaoxjYYQuxoW"
   },
   "source": [
    "## Compare activation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWcdGXoCuxoX",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvYo8vuiuxoX"
   },
   "source": [
    "## Compare embeddings clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdcOhBGxuxoX",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def draw_clustering(embeddings, n_clusters, r=1.5):\n",
    "    clustering = AgglomerativeClustering(n_clusters).fit(np.array(embeddings))\n",
    "    colors = [np.random.rand(3,) for _ in range(n_clusters)]\n",
    "    labels = clustering.labels_\n",
    "    embeddings_2d = MDS().fit_transform(embeddings)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(17, 17))\n",
    "    \n",
    "    for i in range(embeddings_2d.shape[0]):\n",
    "        im = OffsetImage(images[i], zoom=0.1)\n",
    "        ab = AnnotationBbox(im, embeddings_2d[i], xycoords='data', frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        index = labels == i\n",
    "        points = []\n",
    "        for pt in embeddings_2d[index]:\n",
    "            points.append(pt)\n",
    "            points.append(pt + np.array((1, 1)) * r)\n",
    "            points.append(pt + np.array((1, -1)) * r)\n",
    "            points.append(pt + np.array((-1, 1)) * r)\n",
    "            points.append(pt + np.array((-1, -1)) * r)\n",
    "        points = np.array(points)\n",
    "        hull = ConvexHull(points)\n",
    "        hull_points = points[hull.vertices]\n",
    "        x_hull = np.append(hull_points[:, 0], hull_points[:, 0][0])\n",
    "        y_hull = np.append(hull_points[:, 1], hull_points[:, 1][0])\n",
    "        plt.fill(x_hull, y_hull, alpha=0.3, c=colors[i])\n",
    "        \n",
    "    ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "images = images_interesting\n",
    "img_tensors = [transforms(img).unsqueeze(0) for img in images]\n",
    "img_batch = torch.cat(img_tensors)\n",
    "assert img_batch.shape[0] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vit_embeddings = vit.transformer(img_batch)[0][:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "base = list(bit.children())[:-1]\n",
    "pooling = list(bit.head.children())[:-1]\n",
    "bit_pruned = nn.Sequential(*base, *pooling)\n",
    "with torch.no_grad():\n",
    "    bit_embeddings = bit_pruned(img_batch).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "draw_clustering(vit_embeddings, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "draw_clustering(bit_embeddings, 8, r=8)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}