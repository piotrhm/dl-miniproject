{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biw2wtIGuxoO"
   },
   "source": [
    "# ViT vs CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BXqPNcOYA0-h",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from typing import List\n",
    "\n",
    "import BiT\n",
    "from ViT import modeling as ViT\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.Image import Image as Img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xTIMaZBOuxoU"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T0ohcnnTgsdU"
   },
   "outputs": [],
   "source": [
    "def get_weights(path):\n",
    "  return np.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m61ZHyIUuxoV"
   },
   "source": [
    "## Prepare dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (384, 384)\n",
    "NORMALIZE_MEAN = (0.5, 0.5, 0.5)\n",
    "NORMALIZE_STD = (0.5, 0.5, 0.5)\n",
    "transforms = [\n",
    "              T.Resize(IMG_SIZE),\n",
    "              T.ToTensor(),\n",
    "              T.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n",
    "              ]\n",
    "\n",
    "transforms = T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ylaiH85zuxoW"
   },
   "outputs": [],
   "source": [
    "def load_images(dataset_path: str) -> List[Img]:\n",
    "    images = []\n",
    "    for filename in glob.glob(dataset_path + '/*'):\n",
    "        im=Image.open(filename).convert('RGB')\n",
    "        images.append(im)\n",
    "    return images\n",
    "\n",
    "\n",
    "def load_vit(model_name='ViT-B_16', path='imagenet21k+imagenet2012_ViT-B_16.npz') -> nn.Module:\n",
    "    config = ViT.CONFIGS[model_name]\n",
    "    model = ViT.VisionTransformer(config, num_classes=1000, img_size=384, vis=True)\n",
    "    model.load_from(get_weights(path))\n",
    "    return model\n",
    "\n",
    "def load_bit(model_name='BiT-M-R50x3', path='BiT-M-R50x3-ILSVRC2012.npz') -> nn.Module:\n",
    "    model = BiT.KNOWN_MODELS[model_name](head_size=1000)\n",
    "    model.load_from(get_weights(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9oHO_8vSuxoW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piotrhelm/Desktop/IN/miniproj/python/lib/python3.7/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    }
   ],
   "source": [
    "images = load_images(dataset_path)\n",
    "vit = load_vit()\n",
    "bit = load_bit()\n",
    "\n",
    "img = images[1]\n",
    "img_tensor = transforms(img).unsqueeze(0)\n",
    "out_vit = vit(img_tensor)\n",
    "out_bit = bit(img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Wxf-JQ6uxoW"
   },
   "source": [
    "## Compare predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ixCuHDCSuxoW"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaoxjYYQuxoW"
   },
   "source": [
    "## Compare activation maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWcdGXoCuxoX"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvYo8vuiuxoX"
   },
   "source": [
    "## Compare embeddings clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdcOhBGxuxoX"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
